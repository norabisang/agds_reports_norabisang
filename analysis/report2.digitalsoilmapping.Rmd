---
title: "agds_report2_norabisang_digitalsoilmapping"
author: "Nora Bisang"
output: html_document
date: "2025-12-16"
---
## 1. Introduction
This report investigates the spatial prediction of soil waterlogging at 100 cm depth
using digital soil mapping techniques. Random Forest classifiers are trained on
field observations and environmental covariates, with variable selection using
Boruta and optional hyperparameter tuning. Model performance is evaluated on an
independent validation dataset and results are spatially upscaled to the study area.



## 2. Setup and data acquisition
### 2.1 Setup

```{r}
library(here)
library(dplyr)
library(tidyr)
library(readr)
library(purrr)
library(ggplot2)
library(terra)
library(tidyterra)
library(sf)
library(leaflet)
library(knitr)
library(ranger)
library(parallel)
library(visdat)
library(ggplot2)
library(terra)
library(stringr)
```


Loading data
```{r}
df_full <- readRDS(here("data-raw", "df_full.rds"))

# # Load soil data from sampling locations
#get an overview of the df_full 
head(df_full)
names(df_full)

```


Raster listing
```{r}
#now geodata
# Get a list with the path to all raster files
list_raster <- list.files(
  here::here("data-raw/geodata/covariates/"),
  full.names = TRUE
)

# Display file names
map(
  list_raster,
  ~ basename(.)
) |>
  head()

```


```{r}
df_obs <- readr::read_csv(
  here::here("data-raw/soildata/berne_soil_sampling_locations.csv")
  )

# Display data
head(df_obs) |> 
  knitr::kable()
```





This is the map of the sampling locations of our dataset. 
```{r leaflet_sampling_map, echo=FALSE}

# Transform sampling locations from Swiss CRS to WGS84 for mapping
sites_sf <- sf::st_as_sf(
  df_obs,
  coords = c("x", "y"),
  crs = 2056
) |>
  sf::st_transform(4326)

# Split calibration and validation sites
sites_cal <- sites_sf |> dplyr::filter(dataset == "calibration")
sites_val <- sites_sf |> dplyr::filter(dataset == "validation")

# Interactive map for spatial orientation
leaflet::leaflet() |>
  leaflet::addProviderTiles(leaflet::providers$Esri.WorldTopoMap) |>
  leaflet::addCircleMarkers(
    data = sites_cal,
    radius = 4,
    color = "black",
    label = "Calibration sites"
  ) |>
  leaflet::addCircleMarkers(
    data = sites_val,
    radius = 4,
    color = "red",
    label = "Validation sites"
  ) |>
  leaflet::addLegend(
    position = "bottomright",
    colors = c("black", "red"),
    labels = c("Calibration", "Validation"),
    title = "Sampling locations"
  )
```




###2.1 Raster stacking & extraction

Reduction of rasters to the x and y coordinates for which we have soil sampling data. Merging of of soil data and covariates data by their coordinates.

All covariate rasters and sampling coordinates are provided in the Swiss coordinate reference system (CH1903+ / LV95, EPSG:2056), allowing direct extraction of raster values at sampling locations.
```{r}
# Load all files as one batch
all_rasters <- rast(list_raster)

# Extract coordinates from sampling locations
sampling_xy <- df_obs |> 
  dplyr::select(x, y)

# From all rasters, extract values for sampling coordinates
df_covars <- terra::extract(
  all_rasters,  # The raster we want to extract from
  sampling_xy,  # A matrix of x and y values to extract for
  ID = FALSE    # To not add a default ID column to the output
  )

df_full <- cbind(df_obs, df_covars)

head(df_full) |> 
  knitr::kable() 

```



Not all covariates may be continuous. We assume that variables with 10 or less different values are categorical variables.

```{r}
vars_categorical <- df_covars |>
  summarise(across(everything(), ~ n_distinct(.))) |>
  tidyr::pivot_longer(
    everything(),
    names_to = "variable",
    values_to = "n"
  ) |>
  filter(n <= 10) |>
  pull("variable")

cat(
  "Variables with less than 10 distinct values:",
  ifelse(length(vars_categorical) == 0, "none", vars_categorical)
)

```

Mutate categorical columns in our data frame
```{r}
df_full <- df_full |> 
  dplyr::mutate(dplyr::across(all_of(vars_categorical), ~as.factor(.)))

```

Remove what has too many missing variables.
```{r}
# Get number of rows to calculate percentages
n_rows <- nrow(df_full)

# Get number of distinct values per variable
df_full |>
  summarise(across(
    everything(),
    ~ length(.) - sum(is.na(.))
  )) |>
  tidyr::pivot_longer(everything(),
    names_to = "variable",
    values_to = "n"
  ) |>
  mutate(perc_available = round(n / n_rows * 100)) |>
  arrange(perc_available) |>
  head(10) |>
  knitr::kable()

```

Missing data is mostly from the same entries (affects the same observations), by keeping only entries where there is pH data, this will give a dataset with pracitally no missing data and sample size remains sufficient. 




Saving of processed data
```{r}
if (!dir.exists(here::here("data"))) system(paste0("mkdir ", here::here("data")))
saveRDS(df_full, 
        here::here("data/df_full.rds"))
```



##Model training
```{r}
df_full <- readRDS(here("data/df_full.rds"))
head(df_full) |> 
  knitr::kable()
```



Splitting of dataset into a training and a testing set (here already defined "validation" or "calibration"). 
Removal NAs from our training set.

```{r}
# Specify target: waterlogged at 100cm depth
# Make sure target variable is categorical
df_full$waterlog.100 <- factor(
  df_full$waterlog.100,
  levels = c("0", "1")
)
```


' @AddFurtherMetrics not just accuracy below but in confusion matrix i already have. 
Accuracy, Sensitivity, Specificity, Precision, Kappa, Balanced Accuracy what additionally but necessarliy could be added is AUC and F1 


```{r}
target <- "waterlog.100"


# Specify predictors_all: Remove soil sampling and observational data
predictors_all <- names(df_full)[14:ncol(df_full)]

cat("The target is:", target,
    "\nThe predictors_all are:", paste0(predictors_all[1:8], sep = ", "), "...")
```



```{r}
# Split dataset into training and testing sets
df_train <- df_full |> filter(dataset == "calibration")
df_test  <- df_full |> filter(dataset == "validation")

# Filter out any NA to avoid error when running a Random Forest
df_train <- df_train |>  drop_na()

df_test <- df_test |>    drop_na()

# A little bit of verbose output:
n_tot <- nrow(df_train) + nrow(df_test)

perc_cal <- (nrow(df_train) / n_tot) |> round(2) * 100
perc_val <- (nrow(df_test)  / n_tot) |> round(2) * 100

cat("For model training, we have a calibration / validation split of: ",
    perc_cal, "/", perc_val, "%")
```



Balance required after splitting so I think here it is correct. What matters is training balance, not global balance
```{r}
#now i check the balance (to see if special weighing is needed before RF training)
table(df_train$waterlog.100)
prop.table(table(df_train$waterlog.100))
```

###Exercize 1: Interpretation of balance

There is moderate imbalance, but not to severe.

Though calibration dataset shows moderate class imbalance, it is not severe enough to require class weighting.
!!!! NOT 64% BUT: accuracy of the model alone needs to be looked at with caution as it would give ~64% accuracy by just always predicting “0” so more tests could be helpful like : Precision, F1, AUC 



#####################

The model training with default hyperparameters used by ranger::ranger().
```{r}
rf_basic <- ranger(
  y = df_train[, target],     
  x = df_train[, predictors_all], 
   classification = TRUE,
  importance = "permutation",
  seed = 42,
  num.threads = parallel::detectCores() - 1, 
) 


# Print a summary of fitted model
print(rf_basic)

```



##Variable Reduction
In this step the number of variables will be reduced to avoid collinearity and the risk of overfitting.
Further hyperparameters will be optimized for model performance and generalisability improvement. 




Variable importance
```{r}

# Extract the variable importance and create a long tibble
vi_rf_basic <- rf_basic$variable.importance |>
  bind_rows() |>
  pivot_longer(cols = everything(), names_to = "variable")


# Plot variable importance, ordered by decreasing value
gg <- vi_rf_basic |>
  ggplot(aes(x = reorder(variable, value), y = value)) +
  geom_bar(stat = "identity", fill = "grey50", width = 0.75) +
  labs(
    y = "Permutation importance (OOB)",
    x = "",
    title = "RF variable importance (based on OOB)"
  ) +
  theme_classic() +
  coord_flip()

# Display plot
gg


```

Variable Selection
Note that this algorithms don’t assess all possible combinations of predictors_all and may thus not find the “globally” optimal model.
```{r}
#install.packages("Boruta")
library(Boruta)
set.seed(42)


# run the algorithm
bor <- Boruta::Boruta(
    y = df_train[, target], 
    x = df_train[, predictors_all],
    maxRuns = 50, # Set to 30 or lower if it takes too long
    num.threads = parallel::detectCores()-1)


df_bor <- Boruta::attStats(bor) |> 
  tibble::rownames_to_column() |> 
  dplyr::arrange(dplyr::desc(meanImp))

# plot the importance result  
ggplot2::ggplot(ggplot2::aes(x = reorder(rowname, meanImp), 
                             y = meanImp,
                             fill = decision), 
                data = df_bor) +
  ggplot2::geom_bar(stat = "identity", width = 0.75) + 
  ggplot2::scale_fill_manual(values = c("grey30", "tomato", "grey70")) + 
  ggplot2::labs(
    y = "Variable importance", 
    x = "",
    title = "Variable importance based on Boruta") +
  ggplot2::theme_classic() +
  ggplot2::coord_flip()

```


Retaining of the "Confirmed" variables
```{r}
# get retained important variables
predictors_selected <- df_bor |>
  filter(decision == "Confirmed") |>
  pull(rowname)

length(predictors_selected)
```


Retraining random forest with only the "Confirmed" predictor variables
```{r}
# re-train Random Forest model
rf_bor <- ranger(
  y = df_train[, target], # target variable
  x = df_train[, predictors_selected], # Predictor variables
  seed = 42, # Specify the seed for randomization to reproduce the same model again
  num.threads = parallel::detectCores() - 1,
) # Use all but one CPU core for quick model training

# quick report and performance of trained model object
rf_bor

```

Safe files
```{r}
# Save relevant data for model testing in the next chapter.

saveRDS(rf_basic,                   
        here::here("data/rf_for_waterlog_100_basic.rds"))

saveRDS(rf_bor,                   
        here::here("data/rf_for_waterlog_100.rds"))

saveRDS(df_train[, c(target, predictors_all)],
        here::here("data/cal_for_waterlog_100.basic.rds"))
saveRDS(df_test[, c(target, predictors_all)],
        here::here("data/val_for_waterlog_100.basic.rds"))

saveRDS(df_train[, c(target, predictors_selected)],
        here::here("data/cal_for_waterlog_100.rds"))

saveRDS(df_test[, c(target, predictors_selected)],
        here::here("data/val_for_waterlog_100.rds")) 
```


##Model Analysis

The trained model, calibration and validation data are loaded.

##Spatial Upscaling
Load mask of the area over which the soil will be mapped
```{r}
# Load area to be predicted
raster_mask <- rast(here("data-raw/geodata/study_area/area_to_be_mapped.tif"))

# Turn target raster into a dataframe
df_mask <- as.data.frame(raster_mask, xy = TRUE)


# Filter only for area of interest
df_mask <- df_mask |>
  filter(area_to_be_mapped == 1)

# Display df
head(df_mask) |>
  kable()
```



```{r}
files_covariates <- list.files(
  path = here("data-raw/geodata/covariates/"),
  pattern = ".tif$",
  recursive = TRUE,
  full.names = TRUE
)

#to look at it 
random_files <- sample(files_covariates, 2)
rast(random_files[1])
rast(random_files[2])
```



Stacking selected variables
```{r}
# Filter that list only for the variables used in the RF
preds_selected <- rf_bor$forest$independent.variable.names


list_raster <- list.files(
  here("data-raw/geodata/covariates/"),
  full.names = TRUE
)

files_selected <- list_raster |>
  purrr::keep(~ stringr::str_detect(.x, stringr::str_c(preds_selected, collapse = "|")))

# Load all rasters as a stack
raster_covariates <- rast(files_selected)
```


Convert the raster stack into a dataframe 
```{r}
# Get coordinates
df_locations <- df_mask |>
  dplyr::select(x, y)


df_predict <- terra::extract(
  raster_covariates, # The raster we want to extract from
  df_locations, # A matrix of x and y values to extract for
  ID = FALSE # To not add a default ID column to the output
)

df_predict <- cbind(df_locations, df_predict) |>
  tidyr::drop_na() # Se_TWI2m has a small number of missing data

```





##Model Evaluation
###Exercize 2 Skill Comparison
Compare the skill of the models with all predictors_all and with the Boruta-informed reduced set of predictors_all.


```{r}
# Make predictions for validation sites Bor
prediction_bor <- predict(
  rf_bor,
  data = df_test[, predictors_selected],
  num.threads = parallel::detectCores() - 1
)

df_test$pred_boruta_class <- prediction_bor$predictions



prediction_basic <- predict(
  rf_basic,
  data = df_test[, predictors_all],
  num.threads = parallel::detectCores() - 1
)

df_test$pred_basic_class <- prediction_basic$predictions

```


Metrics for comparison of the two models: confusion matrix
```{r}
#boruta model
confusion_bor <- confusionMatrix(
  data = df_test$pred_boruta_class,
  reference = df_test[[target]],
  positive = "1"
)

confusion_bor

#basic model
confusion_basic <- confusionMatrix(
  data = df_test$pred_basic_class,
  reference = df_test[[target]],
  positive = "1"
)

confusion_basic

```

OOB values
```{r}
rf_basic$prediction.error   # OOB error for basic model
rf_bor$prediction.error     # OOB error for Boruta model
```
!!!As concluded above, accuracy alone is insufficient due to class imbalance. Therefore, sensitivity, specificity, precision, balanced accuracy and AUC were considered.


The Boruta-based model generalises better to unseen test data.
It improves the accuracy, kappa, sensitivity, specificity, and balanced accuracy. This is indication that predictor selection by Boruta was useful to remove noisy and irrelevant predictors, improving the performance on unseen data.



The OOB further speaks for better performance of the Boruta model, with a slight decrease in OOB. 
Thus the reduced set will be used for further analysis. !!!OOB error provides an internal estimate of generalisation error and is consistent with validation results.





##Prediction maps and spatial upscaling:

The fitted and tested model will now be used for spatial upscaling - creation of a map of waterlogging at 100cm soil depth across study area.
```{r}
prediction <- predict(
  rf_bor,
  data = df_predict,
  num.threads = parallel::detectCores() - 1
)

df_predict$prediction <- as.factor(prediction$predictions)

# Prepare data for raster
df_map <- df_predict |>
  dplyr::select(x, y, prediction)

# Build raster
raster_pred_bor <- rast(
  df_map,
  crs = "+init=epsg:2056",
  extent = ext(raster_covariates))

```


Visualization
```{r}

levels(raster_pred_bor) <- data.frame(ID = c(0,1), label = c("No","Yes"))

# Plot
ggplot() +
  geom_spatraster(data = raster_pred_bor) +
  scale_fill_viridis_d(
    na.value = NA,
    option = "viridis",
    name = "Waterlogged",
    labels = c("No","Yes")
  ) +
  theme_classic() +
  labs(title = "Waterlogging at 100 cm (Yes / No)")

```



ERRORR ERROR ERROR
Now save as GeoTIFF file. 
```{r}
terra::writeRaster(
   raster_pred_bor,
  here::here("data", "raster_pred_bor.tif"),
   datatype = "FLT4S",  
   filetype = "GTiff",  
   overwrite = TRUE
)

```


##Exercise 3 - Hyperparameter tuning

Hyperparameter tuning of "mtry" and ""min.node.size".
```{r}
#definition train and test sets
x_train <- as.data.frame(df_train[, predictors_selected])
x_test <- as.data.frame(df_test[, predictors_selected])
y_train <- df_train[, target]
y_test <- df_test[, target] 


y_train <- factor(y_train, levels = c("1", "0"), labels = c("Yes", "No"))
y_test  <- factor(y_test,  levels = c("1", "0"), labels = c("Yes", "No")) 



set.seed(12)
rf_grid <- expand.grid(
  mtry = c(2, 5, 6, 10, 15), #how many predictors to try at each split.
  splitrule = "gini",         #split rule for classifications
  min.node.size = c(1,2,3,4,5,10) #here smallest number of samples allowed terminal node
)

ctrl <- trainControl(
  method = "cv",               
  number = 5,
  classProbs = TRUE,           
  summaryFunction = twoClassSummary
)

set.seed(12)
rf_bor_cv <- caret::train(
  x = x_train,
  y = y_train,
  method = "ranger",
  tuneGrid = rf_grid,
  trControl = ctrl,
  importance = "permutation"
)

ggplot(rf_bor_cv)                   
hyperparameters<-rf_bor_cv$bestTune
hyperparameters

```




Evaluation on the test set. I DONT UNDERSTAND THE VALIDATION METRICS HERE
```{r}
library(pROC)

prob_test <- predict(rf_bor_cv, newdata = x_test, type = "prob")

roc_test <- roc(y_test, prob_test[, "Yes"]) #Area under the curve: 0.8115
best_threshold_df<- coords(roc_test, "best", ret = "threshold") 
best_threshold<-best_threshold_df$threshold

pred_test <- ifelse(prob_test[, "Yes"] > best_threshold, "Yes", "No")

cm_test<-confusionMatrix(factor(pred_test, levels = c("Yes", "No")), y_test)
cm_test

mosaicplot(cm_test$table)

```





##Question 3 
###Does the model with adjusted hyperparameters perform better? 
The tuning slightly improved overall performance (mainly in specificity, positive prediction value and balanced accuracy.
However the sensitivity slightly dropped so for cases where missed false positives are more costly that false positives I would consider using the untuned model. 







##Exercize 4 - Fandom Forest: Probability 


Probability of waterlogged soil at 1m depth. 
```{r}
rf_prob <- ranger(
  y = df_train[, target],              
  x = df_train[, predictors_selected], 
  seed = 12, 
  num.threads = parallel::detectCores() - 1, 
  probability= T
) 


rf_prob    

#extraction of the probability of 1
colnames(rf_prob$predictions)
probs_1 <- rf_prob$predictions[, "1"]
```

Reicever-operating-characteristic curve
```{r}
roc_obj <- roc(
  response = df_train[, target],
  predictor = probs_1,
  levels = c("0", "1"),
  direction = "<"
)

# Plot ROC curve
plot(roc_obj, col = "blue", lwd = 2, main = "ROC Curve - RF unadjusted hyperparameters (OOB)")

# AUC
auc(roc_obj)

```








Consider you inform an infrastructure construction project where waterlogged soils severely jeopardize the stability of the building.(1) Then, consider you inform a project where waterlogged soils are unwanted, but not critical. (2) In both cases, your prediction map of a binary classification is used as a basis and the binary classification is derived from the probabilistic prediction. 
4.1 How would you chose the threshold in each case? Would you chose the same threshold in both cases? If not, explain why. 
4.2 Can you think of an analogy of a similarly-natured problem in another realm? (4)



4.1 I would choose the threshold for the cases based on the relative costs/risks of the error types false positive / false negative. For the safety critical building site (case 1) I would choose a lower threshold to avoid false negatives more strictly. For non-critical cases I would choose a higher threshold (to avoid needless and costly intervention). There are algorithms that one can use to estimate appropriate thresholds like the Bayes Decision Theory (BDT). 

4.2 A similar natured problem could be the screening for a severe disease, that needs immediate treatment, where the cost of a false negative (missing a diseased person) would be very big. Whereas for the screening for a non severe, elective issue I would set  the  threshold higher to avoid false positives and unnecessary invasive follow-up.






-----




Train Model with adjusted hyperparameters: 
```{r}
#Now for boruta selected variables
rf_prob_adj <- ranger(
 y = df_train[, target], # target variable
  x = df_train[, predictors_selected],  
  seed = 42,
  num.threads = parallel::detectCores() - 1,
  probability = TRUE,
  mtry = hyperparameters$mtry ,
  splitrule = "gini",
  min.node.size = hyperparameters$min.node.size,
)

# quick report and performance of trained model object
rf_prob_adj

probs_1_adj <- rf_prob_adj$predictions[, "1"]

```


ROC hyperparameters adjusted
```{r}
library(pROC)

roc_obj_adj <- roc(
  response = df_train[, target],
  predictor = probs_1_adj,
  levels = c("0", "1"),
  direction = "<"
)

# Plot ROC curve
plot(roc_obj_adj, col = "blue", lwd = 2, main = "ROC Curve - Hyperparameters Adjusted (OOB)")

# AUC
auc(roc_obj_adj) 


```






Comparison model performance with and without hyperparameter tuning with ROC area under curve. 
```{r}
{plot(roc_obj, col="blue", main="ROC: Hyperparameters Non-adjusted vs adjusted")
lines(roc_obj_adj, col="red") 
legend("bottomright", legend=c("Prob. NA","Prob. A"), col=c("blue","red"), lwd=2)}

```

OOB prediction error
```{r}
rf_prob$prediction.error # OOB error for unadjusted hyperparameters
rf_prob_adj$prediction.error # OOB error for adjusted hyperparameters
```

Interpretation Roc/ AUC
Both models perform well above random and good (chapter 9.4.1 handful of pixels:  AUC takes values between 0 and 1, with 1 being the best and values of 0.5 (=random) and below being bad.)
There is only a very slight improvement of the AUC from the untuned model (AUC: 0.8632) to the model with tuned hyperparameter (AUC: 0.8648). 



We can see that the Boruta selection of parameters is beneficial (slight improvement), while the hyperparameter tuning further but even less strongly improves the performance. Generally we could imply that the hyperparameters chose by default already are quite strong. 


