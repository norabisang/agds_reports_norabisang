---
title: "agds_report2_norabisang_digitalsoilmapping"
output: html_document
date: "2025-12-16"
---
## 1. Introduction


# 2. setup and data acquisition
Setup
```{r}
#install.packages("renv")
library(renv)
library(terra)
library(here)
library(dplyr)
library(readr)
library(purrr)
library(ggplot2)
library(terra)
library(tidyterra)
library(leaflet)
library(sf)
library(here)
library(knitr)
library(dplyr)
library(tidyr)
library(readr)
library(ranger)
library(parallel)
getwd() ##I WANT TO GET OUT OF ANALYSIS

```


Loading data
```{r}
df_full <- readRDS(here("data-raw", "df_full.rds"))
saveRDS(df_full, here::here("data-raw", "df_full.rds"))

# # Load soil data from sampling locations
# readr::write_csv(df_full, "data-raw/df_full.csv") DO I need that CSV?

#get an overview of the df_full 
head(df_full)
names(df_full)

```



```{r}
#now geodata
# Get a list with the path to all raster files
list_raster <- list.files(
  here::here("data-raw/geodata/covariates/"),
  full.names = TRUE
)

# Display file names
map(
  list_raster,
  ~ basename(.)
) |>
  head()

```

```{r}
df_obs <- readr::read_csv(
  here::here("data-raw/soildata/berne_soil_sampling_locations.csv")
  )

# Display data
head(df_obs) |> 
  knitr::kable()
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.



```{r}
# Load a raster file as example: Picking the slope profile at 2 m resolution
#I think i could leave that ALL OUT?!

raster_example <- terra::rast(
  here::here("data-raw/geodata/covariates/Se_slope2m.tif")
  )
raster_example
```
The code chunks filtered for a random sub-sample of 15 variables. As described in Chapter 5, your task will be to investigate all covariates and find the ones that can best be used for your modelling task.



```{r}
Plot raster example
terra::plot(raster_example)
```

To have more flexibility with visualising the data, we can use the ggplot() in combination with the {tidyterra} package.


```{r}
library(tidyterra)

# To have some more flexibility, we can plot this in the ggplot-style as such:
ggplot2::ggplot() +
  tidyterra::geom_spatraster(data = raster_example) +
  ggplot2::scale_fill_viridis_c(
    na.value = NA,
    option = "magma",
    name = "Slope (%) \n"
    ) +
  ggplot2::theme_bw() +
  ggplot2::scale_x_continuous(expand = c(0, 0)) +  # avoid gap between plotting area and axis
  ggplot2::scale_y_continuous(expand = c(0, 0)) +
  ggplot2::labs(title = "Slope of the Study Area")

```
Note that the second plot has different coordinates than the upper one. That is because the data was automatically projected to the World Geodetic System (WGS84, ESPG: 4326).



The code chunk below help to put our map in a bit more context.
Background for orientation and the sampling
A larger map background would be useful to get a better orientation of our location. Also, it would be nice to see where our sampling locations are and to differentiate these locations by whether they are part of the training or testing dataset.
```{r}
# To get our map working correctly, we have to ensure that all the input data
# is in the same coordinate system. Since our Bern data is in the Swiss 
# coordinate system, we have to transform the sampling locations to the 
# World Geodetic System first.
# To look up EPSG Codes: https://epsg.io/
# World Geodetic System 1984:  4326
# Swiss CH1903+ / LV95: 2056

# For the raster:
rasta <- terra::project(raster_example, "+init=EPSG:4326")

# Let's make a function for transforming the sampling locations:
change_coords <- function(data, from_CRS, to_CRS) {
  
  # Check if data input is correct
  if (!all(names(data) %in% c("id", "lat", "lon"))) {
    stop("Input data needs variables: id, lat, lon")
  }
  
  # Create simple feature for old CRS
  sf_old_crs <- sf::st_as_sf(data, coords = c("lon", "lat"), crs = from_CRS)
  
  # Transform to new CRS
  sf_new_crs     <- sf::st_transform(sf_old_crs, crs = to_CRS)
  sf_new_crs$lat <- sf::st_coordinates(sf_new_crs)[, "Y"]
  sf_new_crs$lon <- sf::st_coordinates(sf_new_crs)[, "X"]
  
  sf_new_crs <- sf_new_crs |> dplyr::as_tibble() |> dplyr::select(id, lat, lon)
  
  # Return new CRS
  return(sf_new_crs)
}

# Transform dataframes
coord_train <- df_obs |> 
  dplyr::filter(dataset == "calibration") |> 
  dplyr::select(site_id_unique, x, y) |> 
  dplyr::rename(id = site_id_unique, lon = x, lat = y) |> 
  change_coords(
    from_CRS = 2056, 
    to_CRS = 4326
    )

coord_test <- df_obs |> 
  dplyr::filter(dataset == "validation") |> 
  dplyr::select(site_id_unique, x, y) |> 
  dplyr::rename(id = site_id_unique, lon = x, lat = y) |> 
  change_coords(
    from_CRS = 2056, 
    to_CRS = 4326
    )




# Notes: 
# - This code may only work when installing the development branch of {leaflet}:
#install.packages("remotes")
library(remotes)
#remotes::install_github('rstudio/leaflet')
# - You might have to do library(terra) for R to find functions needed in the backend
library(terra)

# Let's get a nice color palette now for easy reference
pal <- leaflet::colorNumeric(
  "magma",
  terra::values(rasta),           #I here changed r to rasta as defined above
  na.color = "transparent"
  )

# Next, we build a leaflet map
leaflet::leaflet() |> 
  # As base maps, use two provided by ESRI
  leaflet::addProviderTiles(leaflet::providers$Esri.WorldImagery, group = "World Imagery") |>
  leaflet::addProviderTiles(leaflet::providers$Esri.WorldTopoMap, group = "World Topo") |>
  # Add our raster file
  leaflet::addRasterImage(
    rasta,
    colors = pal,
    opacity = 0.6,
    group = "raster"
    ) |>
  # Add markers for sampling locations
  leaflet::addCircleMarkers(
    data = coord_train,
    lng = ~lon,  # Column name for x coordinates
    lat = ~lat,  # Column name for y coordinates
    group = "training",
    color = "black"
  ) |>
    leaflet::addCircleMarkers(
    data = coord_test,
    lng = ~lon,  # Column name for x coordinates
    lat = ~lat,  # Column name for y coordinates
    group = "validation",
    color = "red"
  ) |>
  # Add some layout and legend
  leaflet::addLayersControl(
    baseGroups = c("World Imagery","World Topo"),
    position = "topleft",
    options = leaflet::layersControlOptions(collapsed = FALSE),
    overlayGroups = c("raster", "training", "validation")
    ) |>
  leaflet::addLegend(
    pal = pal,
    values = terra::values(rasta),
    title = "Slope (%)")
```




Now into Data combination
```{r}
# Load all files as one batch
all_rasters <- rast(list_raster) 
all_rasters

```

Above, we have stacked only a random of all available raster data (list_raster) which we have generated previously.

Now, we want to reduce our stacked rasters to the x and y coordinates for which we have soil sampling data. Then, we merge the two dataframes of soil data and covariates data by their coordinates.

```{r}
# Extract coordinates from sampling locations
sampling_xy <- df_obs |> 
  dplyr::select(x, y)

# From all rasters, extract values for sampling coordinates
df_covars <- terra::extract(
  all_rasters,  # The raster we want to extract from
  sampling_xy,  # A matrix of x and y values to extract for
  ID = FALSE    # To not add a default ID column to the output
  )

df_full <- cbind(df_obs, df_covars)

head(df_full) |> 
  knitr::kable() 

```


Now, not all our covariates may be continuous variables and therefore have to be encoded as factors. We check for the number of unique values in each raster. If the variable is continuous, we expect that there are a lot of different values, we assume that variables with 10 or less different values are categorical variables.

```{r}
vars_categorical <- df_covars |>
  # Get number of distinct values per variable
  summarise(across(everything(), ~ n_distinct(.))) |>
  # Turn df into long format for easy filtering
  tidyr::pivot_longer(
    everything(),
    names_to = "variable",
    values_to = "n"
  ) |>
  # Filter out variables with 10 or less distinct values
  filter(n <= 10) |>
  # Extract the names of these variables
  pull("variable")

cat(
  "Variables with less than 10 distinct values:",
  ifelse(length(vars_categorical) == 0, "none", vars_categorical)
)

```

Mutate categorical columns in our data frame
```{r}
df_full <- df_full |> 
  dplyr::mutate(dplyr::across(all_of(vars_categorical), ~as.factor(.)))

```

Remove what has too many missing variables.
```{r}
# Get number of rows to calculate percentages
n_rows <- nrow(df_full)

# Get number of distinct values per variable
df_full |>
  summarise(across(
    everything(),
    ~ length(.) - sum(is.na(.))
  )) |>
  tidyr::pivot_longer(everything(),
    names_to = "variable",
    values_to = "n"
  ) |>
  mutate(perc_available = round(n / n_rows * 100)) |>
  arrange(perc_available) |>
  head(10) |>
  knitr::kable()

#no variable with a substantial amount of missing data. Generally, only pH measurements are lacking, which we should keep in mind when making predictions and inferences


#Another great way to explore your data, is using the {visdat} package:

df_full |> 
  dplyr::select(1:20) |>   # reduce data for readability of the plot
  visdat::vis_miss()
```

We are not missing any data in the covariate data. (Mostly sampled data, pH and timeset data is missing.) 
Missing data is mostly from the same entries, so if we keep only entries where we have pH data, we have a dataset with pracitally no missing data.


SAVE DATA
```{r}
if (!dir.exists(here::here("data"))) system(paste0("mkdir ", here::here("data")))
saveRDS(df_full, 
        here::here("data/df_full.rds"))
```


Model training

```{r}
df_full <- readRDS(here("data/df_full.rds"))
head(df_full) |> 
  knitr::kable()
```



First, we have to specify our target and predictor variables. Then, we have to split our dataset into a training and a testing set (here already defined in df_full$dataset as "validation" or "calibration"). 
Random Forest models cannot deal with NA values, so we have to remove these from our training set.

```{r}
# Specify target: if waterlogged at 100
# Make sure target variable is categorical
df_full$waterlog.100 <- as.factor(df_full$waterlog.100)
levels(df_full$waterlog.100)

#now i check the balance (to see if special weighing is needed before RF training)
table(df_full$waterlog.100)
prop.table(table(df_full$waterlog.100))
```
###Exercize 1: Interpretation of balance

There is moderate imbalance, but not to severe.(like 90/10, or 95/5.)

I can thus train RF classifier w.o. class weighting first.
BUT: accuracy of the model alone needs to be looked at with caution as it would give ~64% accuracy by just always predicting “0” so more tests could be helpful like : Precision, F1, AUC 

' @AddFurtherMetrics not just accuracy below but in confusion matrix i already have. 
Accuracy, Sensitivity, Specificity, Precision, Kappa, Balanced Accuracy what additionally but necessarliy could be added is AUC and F1 


```{r}
target <- "waterlog.100"

#predictors_train <- predictors_all[predictors_all %in% names(df_train)]


# Specify predictors_all: Remove soil sampling and observational data
predictors_all <- names(df_full)[14:ncol(df_full)]

cat("The target is:", target,
    "\nThe predictors_all are:", paste0(predictors_all[1:8], sep = ", "), "...")
```
```{r}
# Split dataset into training and testing sets
df_train <- df_full |> filter(dataset == "calibration")
df_test  <- df_full |> filter(dataset == "validation")

# Filter out any NA to avoid error when running a Random Forest
df_train <- df_train |>  drop_na()

df_test <- df_test |>    drop_na()

# A little bit of verbose (=ausführlich) output:
n_tot <- nrow(df_train) + nrow(df_test)

perc_cal <- (nrow(df_train) / n_tot) |> round(2) * 100
perc_val <- (nrow(df_test)  / n_tot) |> round(2) * 100

cat("For model training, we have a calibration / validation split of: ",
    perc_cal, "/", perc_val, "%")
```

The modelling task is to predict watterlogging at 100cm. Let’s start using the default hyperparameters used by ranger::ranger().
```{r}
?ranger::ranger


# ranger() crashes when using tibbles (a bit like lazy dataframes), so we are using the
# base R notation to enter the data
# are colums missing?
setdiff(predictors_all, colnames(df_train))

library(ranger)
rf_basic <- ranger(
  y = df_train[, target],     # target variable
  x = df_train[, predictors_all], 
  seed = 42,# Specify the seed for randomization to reproduce the same model again
  num.threads = parallel::detectCores() - 1, 
) # Use all but one CPU core for quick model training


# Print a summary of fitted model
print(rf_basic) # we have a classification as factors w. 2 levels #target Node size 1

```

(Note that: If our target variable was a categorical and not a continuous variable, we would have to set the argument probability = TRUE. The output would then be a probability map from 0-100%.)


##Variable Reduction
In this step the number of variables will be reduced to avoid collinearity and the risk of overfitting.
Further hyperparameters will be optimized for model performance and generalisability improvement. 



Our model has 91 variables, but we don’t know anything about their role in influencing the model predictions and how important they are for achieving good predictions.
VARIABLE importance
```{r}
# Let's run the basic model again but with recording the variable importance
rf_basic <- ranger(
  y = df_train[, target],     # target variable
  x = df_train[, predictors_all], # Predictor variables
  importance = "permutation", # Pick permutation to calculate variable importance
  seed = 42, # Specify seed for randomization to reproduce the same model again
  num.threads = parallel::detectCores() - 1, 
  ) # Use all but one CPU core for quick model training

# Extract the variable importance and create a long tibble
vi_rf_basic <- rf_basic$variable.importance |>
  bind_rows() |>
  pivot_longer(cols = everything(), names_to = "variable")


# Plot variable importance, ordered by decreasing value
gg <- vi_rf_basic |>
  ggplot(aes(x = reorder(variable, value), y = value)) +
  geom_bar(stat = "identity", fill = "grey50", width = 0.75) +
  labs(
    y = "Change in OOB MSE after permutation",
    x = "",
    title = "Variable importance based on OOB"
  ) +
  theme_classic() +
  coord_flip()

# Display plot
gg


```

Variable Selection
Note that this algorithms don’t assess all possible combinations of predictors_all and may thus not find the “globally” optimal model.
```{r}
#install.packages("Boruta")
library(Boruta)
set.seed(42)


# run the algorithm
bor <- Boruta::Boruta(
    y = df_train[, target], 
    x = df_train[, predictors_all],
    maxRuns = 50, # Number of iterations. Set to 30 or lower if it takes too long
    num.threads = parallel::detectCores()-1)

# obtain results: a data frame with all variables, ordered by their importance
df_bor <- Boruta::attStats(bor) |> 
  tibble::rownames_to_column() |> 
  dplyr::arrange(dplyr::desc(meanImp))

# plot the importance result  
ggplot2::ggplot(ggplot2::aes(x = reorder(rowname, meanImp), 
                             y = meanImp,
                             fill = decision), 
                data = df_bor) +
  ggplot2::geom_bar(stat = "identity", width = 0.75) + 
  ggplot2::scale_fill_manual(values = c("grey30", "tomato", "grey70")) + 
  ggplot2::labs(
    y = "Variable importance", 
    x = "",
    title = "Variable importance based on Boruta") +
  ggplot2::theme_classic() +
  ggplot2::coord_flip()

```


Retaining of the ("Confirmed") variables
```{r}
# get retained important variables
predictors_selected <- df_bor |>
  filter(decision == "Confirmed") |>
  pull(rowname)

length(predictors_selected)
```
Retraining random forest with only the confirmed predictor variables
```{r}
# re-train Random Forest model
rf_bor <- ranger(
  y = df_train[, target], # target variable
  x = df_train[, predictors_selected], # Predictor variables
  seed = 42, # Specify the seed for randomization to reproduce the same model again
  num.threads = parallel::detectCores() - 1,
) # Use all but one CPU core for quick model training

# quick report and performance of trained model object
rf_bor
?ranger
```

Safe files
```{r}
# Save relevant data for model testing in the next chapter.

saveRDS(rf_basic,                   
        here::here("data/rf_for_waterlog.100.basic.rds"))

saveRDS(rf_bor,                   
        here::here("data/rf_for_waterlog.100.rds"))

saveRDS(df_train[, c(target, predictors_all)],
        here::here("data/cal_for_waterlog.100.basic.rds"))
saveRDS(df_test[, c(target, predictors_all)],
        here::here("data/val_for_waterlog.100.basic.rds"))

saveRDS(df_train[, c(target, predictors_selected)],
        here::here("data/cal_for_waterlog.100.rds"))

saveRDS(df_test[, c(target, predictors_selected)],
        here::here("data/val_for_waterlog.100.rds")) #same also for predictors all?

```


##Model Analysis
```{r}
# Load random forest model
rf_basic   <- readRDS(here::here("data/rf_for_waterlog.100.basic.rds"))
rf_bor   <- readRDS(here::here("data/rf_for_waterlog.100.rds"))
df_train.basic<-readRDS(here::here("data/cal_for_waterlog.100.rds"))

df_test.basic<-readRDS(here::here("data/val_for_waterlog.100.basic.rds"))
df_train.bor <- readRDS(here::here("data/cal_for_waterlog.100.basic.rds"))
df_test.bor  <- readRDS(here::here("data/val_for_waterlog.100.rds"))
```


#Load model and data
The trained model, calibration and validation data are loaded.
```{r}
library(dplyr)
library(tidyr)
library(readr)
library(ggplot2)
library(here)
library(knitr)
library(terra)
library(tidyterra)
library(stringr)
library(purrr)
library(ranger)
library(parallel)
```


```{r}
# Load area to be predicted
raster_mask <- rast(here("data-raw/geodata/study_area/area_to_be_mapped.tif"))

# Turn target raster into a dataframe, 1 px = 1 cell
df_mask <- as.data.frame(raster_mask, xy = TRUE)


# Filter only for area of interest
df_mask <- df_mask |>
  filter(area_to_be_mapped == 1)

# Display df
head(df_mask) |>
  kable()
```



```{r}
files_covariates <- list.files(
  path = here("data-raw/geodata/covariates/"),
  pattern = ".tif$",
  recursive = TRUE,
  full.names = TRUE
)

#to look at it 
random_files <- sample(files_covariates, 2)
rast(random_files[1])
rast(random_files[2])
```



Now we only stack the covariates that we used in RF.bor
```{r}
# Filter that list only for the variables used in the RF
preds_selected2 <- rf_bor$forest$independent.variable.names

list_raster <- list.files(
  here("data-raw/geodata/covariates/"),
  full.names = TRUE
)




files_selected <- list_raster |> 
  purrr::keep(~ str_detect(.x, str_c(preds_selected2, collapse = "|")))

# Load all rasters as a stack
raster_covariates <- rast(files_selected)
```

Convert the raster stack into a dataframe - the preferred format for model prediction.
```{r}
# Get coordinates for which we want data
df_locations <- df_mask |>
  dplyr::select(x, y)

# Extract data from covariate raster stack for all gridcells in the raster
df_predict <- terra::extract(
  raster_covariates, # The raster we want to extract from
  df_locations, # A matrix of x and y values to extract for
  ID = FALSE # To not add a default ID column to the output
)

df_predict <- cbind(df_locations, df_predict) |>
  tidyr::drop_na() # Se_TWI2m has a small number of missing data

```






##Model Evaluation
###Exercize 2
Compare the skill of the models with all predictors_all and with the Boruta-informed reduced set of predictors_all.

To test our model for how well it predicts on data it has not used during model training, we first have to load the {ranger} package to load all functionalities to run a Random Forest with the predict() function. Alongside our model, we feed our validation data into the function and set its parallelization settings to use all but one of our computer’s cores.

```{r}
# Make predictions for validation sites Bor
prediction_bor <- predict(
  rf_bor, # RF model
  data = df_test.bor, # Predictor data
  num.threads = parallel::detectCores() - 1
)

# Save predictions to validation df
df_test.bor$pred_boruta_class <- prediction_bor$predictions



#Same thing for basic model for comparison
prediction_basic <- predict(
  rf_basic, 
  data = df_test.basic, 
  num.threads = parallel::detectCores() - 1
)


# Save predictions
df_test.basic$pred_basic_class <- prediction_basic$predictions

```

Now Metrics for comparison of the 2 models: a confusion matrix for boruta
```{r}
library(caret)
library(lattice)
confusion_bor1 <- confusionMatrix(
  data = df_test.bor$pred_boruta_class,
  reference = df_test.bor$waterlog.100,
  positive = "1"
)

confusion_bor1


#Now same for the basic model
confusion_basic1<- confusionMatrix(
  data = df_test.basic$pred_basic_class,
  reference = df_test.basic$waterlog.100,
  positive = "1"
)

confusion_basic1

```

The Boruta-based model generalises better to unseen test data.
It improves the accuracy, kappa, sensitivity, specificity, and balanced accuracy. This is indication that predictor selection by Boruta was useful to remove noisy and irrelevant predictors, improving the performance on unseen data.


The OOB values
```{r}
rf_basic$prediction.error   # OOB error for full model = 0.214876
rf_bor$prediction.error     # OOB error for Boruta model = 0.2
```
The OOB further speaks for better model performance of the Boruta model, with a slight decrease in OOB. 
Thus the reduced set will be used for further analysis. 





##Prediction maps and spatial upscaling:
The fitted and tested model can now be used for spatial upscaling - creation of a map of waterlogged at 100cm  values across our study area. For this, we again make predictions with our Random Forest model but we use our covariates dataframe for the study area, instead of only at the sampling locations as done above.

```{r}
# Predict (returns class labels because probability=FALSE)
prediction <- predict(
  rf_bor,
  data = df_predict,
  num.threads = parallel::detectCores() - 1
)

# Convert factors to numeric 0/1
df_predict$prediction <- as.numeric(as.character(prediction$predictions))

# Prepare data for raster
df_map <- df_predict |>
  dplyr::select(x, y, prediction)

# Build raster
raster_pred.bor <- rast(
  df_map,
  crs = "+init=epsg:2056",
  extent = ext(raster_covariates)
)

```


Visualization
```{r}
# numeric raster again to factor
raster_pred.bor.factor <- as.factor(raster_pred.bor)

# Levels get name yes no
levels(raster_pred.bor.factor) <- data.frame(ID = c(0,1), label = c("No","Yes"))

# Plot
ggplot() +
  geom_spatraster(data = raster_pred.bor.factor) +
  scale_fill_viridis_d(
    na.value = NA,
    option = "viridis",
    name = "Waterlogged",
    labels = c("No","Yes")
  ) +
  theme_classic() +
  labs(title = "Waterlogging at 100 cm (Yes / No)")

```

Now save as GeoTIFF file. 
```{r}
terra::writeRaster(
  raster_pred.bor.factor,
  "data/raster_pred.bor.factor.tif",
  datatype = "FLT4S",  # FLT4S for floats, INT1U for integers (smaller file)
  filetype = "GTiff",  # GeoTiff format
  overwrite = TRUE     # Overwrite existing file
)
```


-------------------------------------------------------------------------------------

##Exercise 3

Hyperparameter tuning: mtry, min.node.size
```{r}
library(caret)
library(ranger)
library(dplyr)


#definition train and test sets
x_train <- as.data.frame(df_train.bor[, predictors_selected])
x_test <- as.data.frame(df_test.bor[, predictors_selected])
y_train <- as.factor(df_train.bor[, target])
y_test <- df_test.bor[, target]    # factor
#to ensure that 1 is considered positive and 0 considered negative
y_train <- factor(y_train, levels = c("1", "0"), labels = c("Yes", "No"))
y_test  <- factor(y_test,  levels = c("1", "0"), labels = c("Yes", "No")) #for caret



set.seed(12)
rf_grid <- expand.grid(
  mtry = c(2, 5, 6, 10, 15), #how many predictors to try at each split.
  splitrule = "gini",         #split rule for classifications
  min.node.size = c(1,2,3,4,5,10) #here smallest number of samples allowed terminal node
)

ctrl <- trainControl(
  method = "cv",               
  number = 5,
  classProbs = TRUE,           
  summaryFunction = twoClassSummary
)

set.seed(12)
rf_bor_cv <- caret::train(
  x = x_train,
  y = y_train,
  method = "ranger",
  tuneGrid = rf_grid,
  trControl = ctrl,
  importance = "permutation"
)

ggplot(rf_bor_cv)                   #visualization
hyperparameters<-rf_bor_cv$bestTune
hyperparameters

#Balance WAIT WHY DO I HAVE THIS
table(y_train) #0: 358, 1: 247
table(y_test) #0:130, 1 : 70
 

```


Bestes-Tune-results: 
mtry = 10: At each split the algorithm considered 10 random predictors.

splitrule = "gini": Standard Gini impurity used to choose splits.

min.node.size = 3: Terminal nodes allowed to contain as few as 3 observations



Evaluation on the test set
```{r}
library(pROC)

prob.test <- predict(rf_bor_cv, newdata = x_test, type = "prob")

roc_test <- roc(y_test, prob.test[, "Yes"]) #Area under the curve: 0.8115
best_threshold_df<- coords(roc_test, "best", ret = "threshold")
best_threshold<-best_threshold_df$threshold #0.4978333

pred.test <- ifelse(prob.test[, "Yes"] > best_threshold, "Yes", "No")

cm_test<-confusionMatrix(factor(pred.test, levels = c("Yes", "No")), y_test)
cm_test

mosaicplot(cm_test$table)
```
##Question 3 Answer: Does the model with adjusted hyperparameters perform better? 
The tuning slightly improved overall performance (mainly in specificity, positive prediction value and balanced accuracy.
However the sensitivity slightly dropped so for cases where missed false positives are more costly that false positives I would consider using the untuned model. 







##Exercize 4 
Consider you inform an infrastructure construction project where waterlogged soils severely jeopardize the stability of the building.(1) Then, consider you inform a project where waterlogged soils are unwanted, but not critical. (2) In both cases, your prediction map of a binary classification is used as a basis and the binary classification is derived from the probabilistic prediction. 
4.1 How would you chose the threshold in each case? Would you chose the same threshold in both cases? If not, explain why. 
4.2 Can you think of an analogy of a similarly-natured problem in another realm? (4)



4.1 I would choose the threshold for the cases based on the relative costs/risks of the error types false positive / false negative. For the safety critical building site (case 1) I would choose a lower threshold to avoid false negatives more strictly. For non-critical cases I would choose a higher threshold (to avoid needless and costly intervention). There are algorithms that one can use to estimate appropriate thresholds like the Bayes Decision Theory (BDT). 

4.2 A similar natured problem could be the screening for a severe disease, that needs immediate treatment, where the cost of a false negative (missing a diseased person) would be very big. Whereas for the screening for a non severe, elective issue I would set  the  threshold higher to avoid false positives and unnecessary invasive follow-up.





##Probability map
Map of the probability of waterlogged soil at 1m depth. 
```{r}
library(ranger)

rf_prob <- ranger(
  y = df_train[, target],              # target variable
  x = df_train[, predictors_selected], # predictor variables
  seed = 12, 
  num.threads = parallel::detectCores() - 1, # All but one CPU core for quicker model training
  probability= T
) 

# quick report and performance of trained model object
rf_prob    

#extraction of the probability of 1
colnames(rf_prob$predictions)
probs_1 <- rf_prob$predictions[, "1"]
```


```{r}
roc_obj <- roc(
  response = df_train[, target],
  predictor = probs.1,
  levels = c("0", "1"),
  direction = "<"
)

# Plot ROC curve
plot(roc_obj, col = "blue", lwd = 2, main = "ROC Curve - RF unadjusted hyperparameters (OOB)")

# AUC
auc(roc_obj)

```


Train Model with adjusted hyperparameters: 
```{r}
#Now for boruta selected variables
rf_prob_adj <- ranger(
 y = df_train[, target], # target variable
  x = df_train[, predictors_selected],  
  seed = 42,
  num.threads = parallel::detectCores() - 1,
  probability = TRUE,
  mtry = hyperparameters$mtry ,
  splitrule = "gini",
  min.node.size = hyperparameters$min.node.size,
)

# quick report and performance of trained model object
rf_prob_adj

probs_1_adj <- rf_prob_adj$predictions[, "1"]

```


ROC hyperparameters adjusted
```{r}
library(pROC)

roc_obj_adj <- roc(
  response = df_train[, target],
  predictor = probs.1.adj,
  levels = c("0", "1"),
  direction = "<"
)

# Plot ROC curve
plot(roc_obj_adj, col = "blue", lwd = 2, main = "ROC Curve - Hyperparameters Adjusted (OOB)")

# AUC
auc(roc_obj_adj) 


```






Comparison model performance with and without hyperparameter tuning with ROC area under curve. 
```{r}
{plot(roc_obj, col="blue", main="ROC: Hyperparameters Non-adjusted vs adjusted")
lines(roc_obj_adj, col="red") 
legend("bottomright", legend=c("Prob. NA","Prob. A"), col=c("blue","red"), lwd=2)}

```

Interpretation Roc/ AUC
Both models perform well above random and good (chapter 9.4.1 handful of pixels:  AUC takes values between 0 and 1, with 1 being the best and values of 0.5 (=random) and below being bad.)
There is only a very slight improvement of the AUC from the untuned model (AUC: 0.8632) to the model with tuned hyperparameter (AUC: 0.8648). 


OOB prediction error
```{r}
rf_prob$prediction.error # OOB error for unadjusted hyperparameters
rf_prob_adj$prediction.error # OOB error for adjusted hyperparameters
```

We can see that the Boruta selection of parameters is beneficial (slight improvement), while the hyperparameter tuning further but even less strongly improves the performance. Generally we could imply that the hyperparameters chose by default already are quite strong. 




------------------------------------------------------------


A thief just stole some of soil and I'm going after him.





I'm losing ground.