---
title: "agds_report2_norabisang_digitalsoilmapping"
author: "Nora Bisang"
output: html_document
date: "2025-12-16"
---

## 1. Introduction
This report investigates the spatial prediction of soil waterlogging at 100 cm depth
using digital soil mapping techniques. Random Forests are trained on
field observations and environmental covariates, with predictor selection using
Boruta, followed by an optional hyperparameter tuning. Model performance is evaluated on an
independent validation dataset and results are spatially upscaled to the study area.



## 2. Setup and data acquisition
### 2.1 Setup and raw data

```{r}
library(here)
library(dplyr)
library(readr)
library(purrr)
library(ggplot2)
library(terra)
library(tidyterra)
library(sf)
library(leaflet)
library(knitr)
library(ranger)
library(stringr)
library(caret)
library(Boruta)
library(pROC)
```


Loading dataset containing soil observations and site metadata used for model calibration and validation.
```{r}
df_full <- readRDS(here("data-raw", "df_full.rds"))

# # Load soil data from sampling locations
#get an overview of the df_full 
head(df_full)
names(df_full)

```


```{r}
df_obs <- readr::read_csv(
  here::here("data-raw/soildata/berne_soil_sampling_locations.csv")
  )

# Display data
head(df_obs) |> 
  knitr::kable()
```


### 2.2 Covariate Raster "Inventar"

```{r}
# Get a list with the path to all raster files
list_raster <- list.files(
  here::here("data-raw/geodata/covariates/"),
  full.names = TRUE
)

# Display file names
map(
  list_raster,
  ~ basename(.)
) |>
  head()

```


### 2.3 Sampling locations

This map shows the spatial distribution of calibration and validation sites in the study area. A larger base map is used to provide geographic context.
```{r leaflet_sampling_map, echo=FALSE}

# Transform sampling locations from Swiss CRS to WGS84 for mapping
sites_sf <- sf::st_as_sf(
  df_obs,
  coords = c("x", "y"),
  crs = 2056
) |>
  sf::st_transform(4326)

# Split calibration and validation sites
sites_cal <- sites_sf |> dplyr::filter(dataset == "calibration")
sites_val <- sites_sf |> dplyr::filter(dataset == "validation")

# Interactive map for spatial orientation
leaflet::leaflet() |>
  leaflet::addProviderTiles(leaflet::providers$Esri.WorldTopoMap) |>
  leaflet::addCircleMarkers(
    data = sites_cal,
    radius = 4,
    color = "black",
    label = "Calibration sites"
  ) |>
  leaflet::addCircleMarkers(
    data = sites_val,
    radius = 4,
    color = "red",
    label = "Validation sites"
  ) |>
  leaflet::addLegend(
    position = "bottomright",
    colors = c("black", "red"),
    labels = c("Calibration", "Validation"),
    title = "Sampling locations"
  )
```




## 3. Covariate preparation

### 3.1 Raster stacking
Reduction of rasters to the x and y coordinates for which we have soil sampling data.
All covariate rasters and sampling coordinates are provided in the Swiss coordinate reference system (CH1903+ / LV95, EPSG:2056), allowing direct extraction of raster values at sampling locations.
```{r}
# Stack all environmental covariates into a single raster
all_rasters <- rast(list_raster)

# Extract coordinates from sampling locations
sampling_xy <- df_obs |> 
  dplyr::select(x, y)
```

### 3.2 Extraction at sampling locations
Merging of of soil data and covariates data by their coordinates.
```{r}
# From all rasters, extract values for sampling coordinates
df_covars <- terra::extract(
  all_rasters,  
  sampling_xy, 
  ID = FALSE   
  )

df_full <- cbind(df_obs, df_covars)

head(df_full) |> 
  knitr::kable() 

```


### 3.3 Detection categroical variables
Variables with 10 or less different values are treated as categorical, assuming they represent classes rather than continuous gradients.
```{r}
vars_categorical <- df_covars |>
  summarise(across(everything(), ~ n_distinct(.))) |>
  tidyr::pivot_longer(
    everything(),
    names_to = "variable",
    values_to = "n"
  ) |>
  filter(n <= 10) |>
  pull("variable")

cat(
  "Variables with less than 10 distinct values:",
  ifelse(length(vars_categorical) == 0, "none", vars_categorical)
)

```

Mutate categorical columns in our data frame
```{r}
df_full <- df_full |> 
  dplyr::mutate(dplyr::across(all_of(vars_categorical), ~as.factor(.)))

```

### 3.4 Missing Data
Missing data is examined by computing the percentage of non-missing observations for each predictor variable. For subsequent analysis dataset can be used without removal of variable, as the proportion of missing values is low.
```{r}
n_rows <- nrow(df_full)

# number of distinct values per variable
df_full |>
  summarise(across(
    everything(),
    ~ length(.) - sum(is.na(.))
  )) |>
  tidyr::pivot_longer(everything(),
    names_to = "variable",
    values_to = "n"
  ) |>
  mutate(perc_available = round(n / n_rows * 100)) |>
  arrange(perc_available) |>
  head(10) |>
  knitr::kable()

```


### 3.5 Saving of processed data
```{r}
if (!dir.exists(here::here("data"))) system(paste0("mkdir ", here::here("data")))
saveRDS(df_full, 
        here::here("data/df_full.rds"))
```



## 4. Model training


### 4.1 Target definition
```{r}
target <- "waterlog.100"
```


### 4.2 Predictor definition
```{r}
# Specify predictors_all: Remove soil sampling and observational data
predictors_all <- names(df_full)[14:ncol(df_full)]

cat("The target is:", target,
    "\nThe predictors_all are:", paste0(predictors_all[1:8], sep = ", "), "...")
```


### 4.3 Splitting dataset train/ test
Splitting of the dataset into calibration and validation subsets (based on present labels) to ensure spatial independence for evaluation.
```{r}
# Split dataset into training and testing sets
df_train <- df_full |> filter(dataset == "calibration")
df_test  <- df_full |> filter(dataset == "validation")

# Filter out any NA to avoid error when running a Random Forest
df_train <- df_train |>  drop_na()

df_test <- df_test |>    drop_na()

# A little bit of verbose output:
n_tot <- nrow(df_train) + nrow(df_test)

perc_cal <- (nrow(df_train) / n_tot) |> round(2) * 100
perc_val <- (nrow(df_test)  / n_tot) |> round(2) * 100

cat("For model training, we have a calibration / validation split of: ",
    perc_cal, "/", perc_val, "%")
```


### 4.4 Exercise 1 – Class balance

```{r}
table(df_train$waterlog.100)
prop.table(table(df_train$waterlog.100))
```

Interpretation of Class Balance

The training dataset shows moderate class imbalance. From 605 observations, 358 (59.2 %) are in the not-waterlogged class, while 247 observations (40.8 %) are in the waterlogged class.

This imbalance is not severe and both classes are still well represented. Therefore class weighting will notbe applied during model training.
 
However, as predicting only the majority class "not-waterlogged" would already achieve an accuracy of 59 %, accuracy alone is not a sufficient performance metric. This is why the model evaluation will focus not only on accuracy, but also on sensitivity, specificity, precision, balanced accuracy, and AUC, which are more informative under class imbalance.



## 4.5 Random Forest basic model

Model training with default hyperparameters used by ranger::ranger().
```{r}
rf_basic <- ranger(
  y = df_train[, target],     
  x = df_train[, predictors_all], 
   classification = TRUE,
  importance = "permutation",
  seed = 42,
  num.threads = parallel::detectCores() - 1, 
) 

# Print a summary of fitted model
print(rf_basic)

```


## 5. Variable selection using Boruta

The number of variables will be reduced to avoid collinearity and the risk of overfitting.


### 5.1 Extraction of Variable importance
```{r}
# Extraction the variable importance 
vi_rf_basic <- rf_basic$variable.importance |>
  bind_rows() |>
  pivot_longer(cols = everything(), names_to = "variable")


# Plot variable importance, ordered by decreasing value
gg <- vi_rf_basic |>
  ggplot(aes(x = reorder(variable, value), y = value)) +
  geom_bar(stat = "identity", fill = "grey50", width = 0.75) +
  labs(
    y = "Permutation importance (OOB)",
    x = "",
    title = "RF variable importance (based on OOB)"
  ) +
  theme_classic() +
  coord_flip()

# Display plot
gg


```

### 5.2 Variable Selection
Note that this algorithms does not assess all possible combinations of predictors_all and may thus not find the “globally” optimal model.
```{r}
#install.packages("Boruta")
library(Boruta)
set.seed(42)


# run the algorithm
bor <- Boruta::Boruta(
    y = df_train[, target], 
    x = df_train[, predictors_all],
    maxRuns = 50, # Set to 30 or lower if it takes too long
    num.threads = parallel::detectCores()-1)


df_bor <- Boruta::attStats(bor) |> 
  tibble::rownames_to_column() |> 
  dplyr::arrange(dplyr::desc(meanImp))

# plot the importance result  
ggplot2::ggplot(ggplot2::aes(x = reorder(rowname, meanImp), 
                             y = meanImp,
                             fill = decision), 
                data = df_bor) +
  ggplot2::geom_bar(stat = "identity", width = 0.75) + 
  ggplot2::scale_fill_manual(values = c("grey30", "tomato", "grey70")) + 
  ggplot2::labs(
    y = "Variable importance", 
    x = "",
    title = "Variable importance based on Boruta") +
  ggplot2::theme_classic() +
  ggplot2::coord_flip()

```


### 5.3 Retiring of the confirmed variable
```{r}
# get retained important variables
predictors_selected <- df_bor |>
  filter(decision == "Confirmed") |>
  pull(rowname)

length(predictors_selected)
```

### 5.4 Retaining RF with the "Confirmed" variables
Retraining random forest with only the "Confirmed" predictor variables
```{r}
# re-train Random Forest model
rf_bor <- ranger(
  y = df_train[, target], # target variable
  x = df_train[, predictors_selected], # Predictor variables
  seed = 42, # Specify the seed for randomization to reproduce the same model again
  num.threads = parallel::detectCores() - 1,
) # Use all but one CPU core for quick model training

# quick report and performance of trained model object
rf_bor

```

### 5.5 Safe files
```{r}
saveRDS(rf_basic,                   
        here::here("data/rf_for_waterlog_100_basic.rds"))
saveRDS(rf_bor,                   
        here::here("data/rf_for_waterlog_100.rds"))

saveRDS(df_train[, c(target, predictors_all)],
        here::here("data/cal_for_waterlog_100.basic.rds"))
saveRDS(df_test[, c(target, predictors_all)],
        here::here("data/val_for_waterlog_100.basic.rds"))

saveRDS(df_train[, c(target, predictors_selected)],
        here::here("data/cal_for_waterlog_100.rds"))
saveRDS(df_test[, c(target, predictors_selected)],
        here::here("data/val_for_waterlog_100.rds")) 
```


## 6. Model Analysis
### Exercize 2 Model Comparison
Compare the skill of the models with all predictors and with the Boruta-informed reduced set of predictors.

###6.1 Predictions
```{r}
prediction_bor <- predict(
  rf_bor,
  data = df_test[, predictors_selected],
  num.threads = parallel::detectCores() - 1
)

df_test$pred_boruta_class <- prediction_bor$predictions

# Align factor levels between predictions and reference to avoid caret warnings
df_test$pred_boruta_class <- factor(
  df_test$pred_boruta_class,
  levels = levels(df_test[[target]])
)



prediction_basic <- predict(
  rf_basic,
  data = df_test[, predictors_all],
  num.threads = parallel::detectCores() - 1
)

df_test$pred_basic_class <- prediction_basic$predictions
df_test$pred_basic_class <- factor(
  df_test$pred_basic_class,
  levels = levels(df_test[[target]])
)

```


### 6.2 Confusion matrixes
```{r}
confusion_bor <- confusionMatrix(
  data = df_test$pred_boruta_class,
  reference = df_test[[target]],
  positive = "1"
)

confusion_bor

#basic model
confusion_basic <- confusionMatrix(
  data = df_test$pred_basic_class,
  reference = df_test[[target]],
  positive = "1"
)

confusion_basic

```

### 6.3 OOB 
```{r}
rf_basic$prediction.error   
rf_bor$prediction.error    
```


### 6.4 Interpretation
The confusion matrixes show that the predictor reduced model (rf_bor) consistently outperforms the basic model (rf_basic) across all shown metrics.

The OOB further speaks for better performance of the Boruta model, with a decrease of ~1.7% in OOB. 

The outperformance of the reduced predicot model is indication that predictor selection by Boruta was useful to remove noisy and irrelevant predictors, improving the performance on unseen data (generaliseability).
Thus the reduced set will be used for further analysis.




## 7. Spatial Upscaling

### 7.1  Mask loading
```{r}
raster_mask <- rast(here("data-raw/geodata/study_area/area_to_be_mapped.tif"))

# Turn target raster into a dataframe
df_mask <- as.data.frame(raster_mask, xy = TRUE)

df_mask <- df_mask |>
  filter(area_to_be_mapped == 1)

head(df_mask) |>
  kable()
```


### 7.2 Loading covariates
```{r}
files_covariates <- list.files(
  path = here("data-raw/geodata/covariates/"),
  pattern = ".tif$",
  recursive = TRUE,
  full.names = TRUE
)

#overview
random_files <- sample(files_covariates, 2)
rast(random_files[1])
rast(random_files[2])
```



### 7.3 Stacking of selected variables
```{r}
preds_selected <- rf_bor$forest$independent.variable.names


list_raster <- list.files(
  here("data-raw/geodata/covariates/"),
  full.names = TRUE
)

files_selected <- list_raster |>
  purrr::keep(~ stringr::str_detect(.x, stringr::str_c(preds_selected, collapse = "|")))

# Load all rasters as a stack
raster_covariates <- rast(files_selected)
```


### 7.4 Conversion raster stack to dataframe 
```{r}
#Coordinates
df_locations <- df_mask |>
  dplyr::select(x, y)


df_predict <- terra::extract(
  raster_covariates, 
  df_locations, 
  ID = FALSE 
)

df_predict <- cbind(df_locations, df_predict) |>
  tidyr::drop_na() 

```


## 8. Spatial prediction and mapping
The fitted and tested model will now be used for spatial upscaling - creation of a map of waterlogging at 100cm soil depth across study area.

### 8.1 Pixel-wise prediction
```{r}
prediction <- predict(
  rf_bor,
  data = df_predict,
  num.threads = parallel::detectCores() - 1
)

df_predict$prediction <- factor(
  prediction$predictions,
  levels = c("0", "1"),
  labels = c("No", "Yes")
)
```

### 8.2 Raster reconstruction
```{r}
# Prepare data for raster
df_map <- df_predict |>
  dplyr::select(x, y, prediction)

# Convert prediction dataframe to raster
raster_pred_bor <- rast(
  df_map,
  crs = "+init=epsg:2056",     
  extent = ext(raster_covariates))

```


### 8.3 Map visualization
```{r}
ggplot() +
  geom_spatraster(data = raster_pred_bor) +
  scale_fill_viridis_d(
    na.value = NA,
    option = "viridis",
    name = "Waterlogged",
    labels = c("No","Yes")
  ) +
  theme_classic() +
  labs(title = "Predicted waterlogging at 100 cm depth")

```



### 8.4 Save GeoTIFF
```{r}
terra::writeRaster(
   raster_pred_bor,
  here::here("data", "raster_pred_bor.tif"),
   datatype = "FLT4S",  
   filetype = "GTiff",  
   overwrite = TRUE
)

```



## 9 Hyperparameter tuning - Exercise 3 

```{r}
# Select predictors
x_train <- as.data.frame(df_train[, predictors_selected])
x_test  <- as.data.frame(df_test[, predictors_selected])

# Recode target so that the positive class is alphabetically first
# (caret requirement for ROC / AUC calculation)
y_train <- factor(
  ifelse(df_train[[target]] == "1", "A_Yes", "B_No")
)

y_test <- factor(
  ifelse(df_test[[target]] == "1", "A_Yes", "B_No")
)
```


### 9.2 Grid tuning
Hyperparameter tuning of "mtry" and ""min.node.size".
While the default "minimum node size" for classification is 1, I restricted the tuning grid to values of 2 and above to ensure generaliseability and reduce the risk of the individual trees overfitting to noise in the training set.
```{r}
set.seed(12)
rf_grid <- expand.grid(
  mtry = c(2, 5, 10), 
  splitrule = "gini",         
  min.node.size = c(2,5,10) 
)

ctrl <- trainControl(
  method = "cv",               
  number = 5,
  classProbs = TRUE,           
  summaryFunction = twoClassSummary
)
```

### 9.3 Model training
```{r}
set.seed(12)
rf_bor_cv <- caret::train(
  x = x_train,
  y = y_train,
  method = "ranger",
  tuneGrid = rf_grid,
  trControl = ctrl,
  importance = "permutation",
   metric = "ROC"
)

ggplot(rf_bor_cv)                   
hyperparameters<-rf_bor_cv$bestTune
hyperparameters

```



## 10. Evaluation on independent validation set
Evaluation on the test set. I DONT UNDERSTAND THE VALIDATION METRICS HERE
```{r}
# Predict class probabilities on validation data
prob_test <- predict(rf_bor_cv, newdata = x_test, type = "prob")
```

### 10.2 ROC curve based on probabilities
```{r}
roc_test <- roc(y_test, prob_test[, "A_Yes"]) 
best_threshold_df<- coords(roc_test, "best", ret = "threshold") 
best_threshold<-best_threshold_df$threshold

# Convert probabilities to classes using optimal threshold
pred_test <- ifelse(prob_test[, "A_Yes"] > best_threshold, "A_Yes", "B_No")

pred_test <- factor(pred_test, levels = levels(y_test))

# Confusion matrix
cm_test<-confusionMatrix(factor(pred_test, levels = c("A_Yes", "B_No")), y_test)

cm_test

mosaicplot(cm_test$table)

```





##Question 3 
###Does the model with adjusted hyperparameters perform better? 
!!!Validation metrics are derived from thresholded probability predictions.
The ROC curve evaluates discrimination ability independently of a fixed threshold, while the confusion matrix summarizes classification performance at the selected operating point.


The tuning slightly improved overall performance (mainly in specificity, positive prediction value and balanced accuracy.
However the sensitivity slightly dropped so for cases where missed false positives are more costly that false positives I would consider using the untuned model. 






## 10. Random Forest probability model (Exercise 4)

### 10.1 Probability RF model
```{r}
rf_prob <- ranger(
  y = df_train[, target],              
  x = df_train[, predictors_selected], 
  seed = 12, 
  num.threads = parallel::detectCores() - 1, 
  probability= T
) 


rf_prob    

#extraction of the probability of 1
probs_1 <- rf_prob$predictions[, "1"]
```

### 10.2 ROC and AUC analysis (OOB based)
```{r}
roc_obj <- roc(
  response = df_train[, target],
  predictor = probs_1,
  levels = c("0", "1"),
  direction = "<"
)

# Plot ROC curve
plot(roc_obj, col = "blue", lwd = 2, main = "ROC Curve - RF unadjusted hyperparameters (OOB)")

# AUC
auc(roc_obj)

```








Consider you inform an infrastructure construction project where waterlogged soils severely jeopardize the stability of the building.(1) Then, consider you inform a project where waterlogged soils are unwanted, but not critical. (2) In both cases, your prediction map of a binary classification is used as a basis and the binary classification is derived from the probabilistic prediction. 
4.1 How would you chose the threshold in each case? Would you chose the same threshold in both cases? If not, explain why. 
4.2 Can you think of an analogy of a similarly-natured problem in another realm? (4)



4.1 I would choose the threshold for the cases based on the relative costs/risks of the error types false positive / false negative. For the safety critical building site (case 1) I would choose a lower threshold to avoid false negatives more strictly. For non-critical cases I would choose a higher threshold (to avoid needless and costly intervention). There are algorithms that one can use to estimate appropriate thresholds like the Bayes Decision Theory (BDT). 

4.2 A similar natured problem could be the screening for a severe disease, that needs immediate treatment, where the cost of a false negative (missing a diseased person) would be very big. Whereas for the screening for a non severe, elective issue I would set  the  threshold higher to avoid false positives and unnecessary invasive follow-up.







Train Probability Model with adjusted hyperparameters: 
```{r}
#Now for boruta selected variables
rf_prob_adj <- ranger(
 y = df_train[, target], # target variable
  x = df_train[, predictors_selected],  
  seed = 42,
  num.threads = parallel::detectCores() - 1,
  probability = TRUE,
  mtry = hyperparameters$mtry ,
  splitrule = "gini",
  min.node.size = hyperparameters$min.node.size,
)

# quick report and performance of trained model object
rf_prob_adj

probs_1_adj <- rf_prob_adj$predictions[, "1"]

```


ROC hyperparameters adjusted
```{r}
roc_obj_adj <- roc(
  response = df_train[, target],
  predictor = probs_1_adj,
  levels = c("0", "1"),
  direction = "<"
)

# Plot ROC curve
plot(roc_obj_adj, col = "blue", lwd = 2, main = "ROC Curve - Hyperparameters Adjusted (OOB)")

# AUC
auc(roc_obj_adj) 


```






Comparison model performance with and without hyperparameter tuning with ROC area under curve. 
```{r}
{plot(roc_obj, col="blue", main="ROC: Hyperparameters Non-adjusted vs adjusted")
lines(roc_obj_adj, col="red") 
legend("bottomright", legend=c("Prob. NA","Prob. A"), col=c("blue","red"), lwd=2)}

```

OOB prediction error
```{r}
rf_prob$prediction.error # OOB error for unadjusted hyperparameters
rf_prob_adj$prediction.error # OOB error for adjusted hyperparameters
```

Interpretation Roc/ AUC
Both models perform well above random and good (chapter 9.4.1 handful of pixels:  AUC takes values between 0 and 1, with 1 being the best and values of 0.5 (=random) and below being bad.)
There is only a very slight improvement of the AUC from the untuned model (AUC: 0.8632) to the model with tuned hyperparameter (AUC: 0.8648). 



We can see that the Boruta selection of parameters is beneficial (slight improvement), while the hyperparameter tuning further but even less strongly improves the performance. Generally we could imply that the hyperparameters chose by default already are quite strong. 


